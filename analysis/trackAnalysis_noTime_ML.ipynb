{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patheffects as pe\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.path import Path\n",
    "import uproot\n",
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.spatial.distance import cdist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2 as cv\n",
    "from joblib import Parallel, delayed\n",
    "import h5py\n",
    "\n",
    "# %matplotlib qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../python/')\n",
    "\n",
    "from filterMethods import *\n",
    "from timeMethods import *\n",
    "from gridMethods import *\n",
    "from trackReconstructionMethods import *\n",
    "from vertexReconstructionMethods import *\n",
    "from hitAccuracyMethods import *\n",
    "from statisticsMethods import *\n",
    "from importMethods import *\n",
    "from constants import *\n",
    "from plotMethods import *\n",
    "from UNetMethods import *\n",
    "\n",
    "import plotParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "useHistograms = True # if True use histograms, if False use tupple data\n",
    "\n",
    "checkFiles = False # check if run ROOT files (1) exit, (2) have enough hits, (3) have enough primary particle steps, (4) all DSPS hit histograms are the same size\n",
    "\n",
    "dataDir = '/Users/noah-everett/Documents_non-iCloud/DSPS/'\n",
    "tmpDir  = dataDir+'tmp/'\n",
    "\n",
    "fileNumbers = [i for i in range(35)] # the ML data set has 1138 files\n",
    "runDataRoot_filePaths = ['/Users/noah-everett/Documents/FNAL/Geant4/USSD_Geant4/runs/MLtrainData/multievent_{}.root'.format(i) for i in fileNumbers]\n",
    "runDataRoot_hits_histDir = '/photoSensor_hits_histograms'\n",
    "runDataRoot_hits_treeName = 'photoSensor_hits;1'\n",
    "runDataRoot_primary_treeName = 'primary;1'\n",
    "\n",
    "saveDf_hits                = True\n",
    "saveDf_primary             = True\n",
    "saveDfOnlyIfNoFile_hits    = True\n",
    "saveDfOnlyIfNoFile_primary = True\n",
    "saveDf_hits_basePath       = tmpDir+'simDF/MLdata_DF_hits'    # directory and beginning of file name\n",
    "saveDf_primary_basePath    = tmpDir+'simDF/MLdata_DF_primary' # directory and beginning of file name\n",
    "\n",
    "saveGrid_hits_npy                = False\n",
    "saveGrid_primary_npy             = False\n",
    "saveGrids_h5                     = True\n",
    "saveGridOnlyIfNoFile_hits_npy    = True\n",
    "saveGridOnlyIfNoFile_primary_npy = True\n",
    "saveGridOnlyIfNoFile_h5          = True\n",
    "saveGrid_hits_basePath_npy       = tmpDir+'MLdata/MLdata_grid_hits'    # directory and beginning of file name\n",
    "saveGrid_primary_basePath_npy    = tmpDir+'MLdata/MLdata_grid_primary' # directory and beginning of file name\n",
    "saveGrids_h5                     = tmpDir+'MLdata/MLdata_grids'        # directory and beginning of file name\n",
    "\n",
    "saveModel          = True\n",
    "saveModel_basePath = 'data/MLmodels/MLmodel' # directory and beginning of file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(runDataRootPaths): 35\n"
     ]
    }
   ],
   "source": [
    "minHits = 0\n",
    "minPrimarySteps = 30\n",
    "\n",
    "if (saveDf_hits or saveDf_primary) and checkFiles:\n",
    "    DSPSsizes = [get_histogram_sizes(fileName, runDataRoot_hits_histDir) for fileName in tqdm.tqdm(runDataRoot_filePaths)]\n",
    "    assert len(np.unique(np.array(DSPSsizes).reshape(-1, 2))) >  0, 'No data found?'\n",
    "    assert len(np.unique(np.array(DSPSsizes).reshape(-1, 2))) == 1, 'Different sizes: {}'.format(np.unique(DSPSsizes))\n",
    "\n",
    "    nHits = [get_histogram_nHits_total(fileName, runDataRoot_hits_histDir) for fileName in tqdm.tqdm(runDataRoot_filePaths)]\n",
    "    nSteps = [len(get_primary_pdg(fileName, runDataRoot_primary_treeName)) for fileName in tqdm.tqdm(runDataRoot_filePaths)]\n",
    "\n",
    "    fileNumbers     = [fileNumber for fileNumber, nHit in zip(fileNumbers          , nHits) if nHit > minHits]\n",
    "    runDataRootPath = [fileName   for fileName  , nHit in zip(runDataRoot_filePaths, nHits) if nHit > minHits]\n",
    "\n",
    "    fileNumbers     = [fileNumber for fileNumber, nStep in zip(fileNumbers          , nSteps) if nStep > minPrimarySteps]\n",
    "    runDataRootPath = [fileName   for fileName  , nStep in zip(runDataRoot_filePaths, nSteps) if nStep > minPrimarySteps]\n",
    "\n",
    "print('len(runDataRootPaths):', len(runDataRoot_filePaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaths_hits    = [f\"{saveDf_hits_basePath}_{i}.parquet\"    for i in fileNumbers]\n",
    "dfPaths_primary = [f\"{saveDf_primary_basePath}_{i}.parquet\" for i in fileNumbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 44934.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def rToTheta(r):\n",
    "    return r/(CM_PER_RAD*MM_PER_CM)\n",
    "\n",
    "if saveDf_hits:\n",
    "    for i, (fileName, dfPath_hits) in tqdm.tqdm(enumerate(zip(runDataRoot_filePaths, dfPaths_hits)), total=len(runDataRoot_filePaths)):\n",
    "        if saveDfOnlyIfNoFile_hits and os.path.exists(dfPath_hits):\n",
    "            continue\n",
    "        df_hits = pd.DataFrame(columns=['sensor_name', 'sensor_direction', 'sensor_position', 'sensor_wall', 'relativePosition_binned', 'relativePosition_nBin'])\n",
    "        if useHistograms:\n",
    "            photosensor_IDs, photosensor_directions, photosensor_positions, photosensor_walls, position_relative_binned, position_relative_nBin = get_histogram_hits_tuple(fileName, runDataRoot_hits_histDir, True)\n",
    "            df_hits['sensor_name'            ] = photosensor_IDs\n",
    "            df_hits['sensor_direction'       ] = photosensor_directions\n",
    "            df_hits['sensor_position'        ] = photosensor_positions\n",
    "            df_hits['sensor_wall'            ] = photosensor_walls\n",
    "            df_hits['relativePosition_binned'] = position_relative_binned\n",
    "            df_hits['relativePosition_nBin'  ] = position_relative_nBin\n",
    "        else:\n",
    "            df_hits['sensor_name'            ] = get_photosensor_hits_photosensor_ID          (fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            df_hits['sensor_direction'       ] = get_photosensor_hits_photosensor_direction   (fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            df_hits['sensor_position'        ] = get_photosensor_hits_photosensor_position    (fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            df_hits['sensor_wall'            ] = get_photosensor_hits_photosensor_wall        (fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            # df_hits['relativePosition'       ] = get_photosensor_hits_position_relative       (fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            df_hits['relativePosition_binned'] = get_photosensor_hits_position_relative_binned(fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            df_hits['relativePosition_nBin'  ] = get_photosensor_hits_position_relative_nBin  (fileName, runDataRoot_hits_treeName, runDataRoot_hits_histDir, verbose=False)\n",
    "            df_hits['initialPosition'        ] = get_photosensor_hits_position_initial        (fileName, runDataRoot_hits_treeName,          verbose=False)\n",
    "\n",
    "        df_hits = make_r                            (df_hits)\n",
    "        df_hits = filter_r                          (df_hits, Y_LIM)\n",
    "        df_hits = make_theta                        (df_hits, rToTheta)\n",
    "        df_hits = make_phi                          (df_hits)\n",
    "        df_hits = make_reconstructedVector_direction(df_hits)\n",
    "        if 'initialPosition' in df_hits.columns:\n",
    "            df_hits = make_relativeVector(df_hits)\n",
    "    \n",
    "        df_hits.to_parquet(dfPath_hits, compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:01<00:00, 28.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def processPrimaryFile(fileName, treeName, dfPath_primary):\n",
    "    if saveDfOnlyIfNoFile_primary and os.path.exists(dfPath_primary):\n",
    "        return\n",
    "\n",
    "    df_primary = pd.DataFrame(columns=['position', 'pdg'])\n",
    "    df_primary['position' ] = get_primary_position(fileName, treeName)\n",
    "    df_primary['pdg'      ] = get_primary_pdg     (fileName, treeName)\n",
    "    df_primary = df_primary[df_primary['pdg'] == 13] # muon\n",
    "    df_primary = df_primary[df_primary['position'].apply(lambda x:\n",
    "        x[0] > -DETECTOR_SIZE_MM[0]/2 and x[0] < DETECTOR_SIZE_MM[0]/2 and\n",
    "        x[1] > -DETECTOR_SIZE_MM[1]/2 and x[1] < DETECTOR_SIZE_MM[1]/2 and\n",
    "        x[2] > -DETECTOR_SIZE_MM[2]/2 and x[2] < DETECTOR_SIZE_MM[2]/2\n",
    "    )]\n",
    "\n",
    "    df_primary.to_parquet(dfPath_primary, compression='snappy')\n",
    "\n",
    "args_list = [(fileName, runDataRoot_primary_treeName, dfPath_primary) for fileName, dfPath_primary in zip(runDataRoot_filePaths, dfPaths_primary)]\n",
    "\n",
    "if saveDf_primary:\n",
    "    Parallel(n_jobs=4)(delayed(processPrimaryFile)(*args) for args in tqdm.tqdm(args_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make or Load Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80\n",
    "gridSize_input  = (N, N, N)\n",
    "gridSize_output = (N, N, N)\n",
    "\n",
    "voxelGrid_make_errors               = False\n",
    "voxelGrid_make_walls                = True\n",
    "voxelGrid_make_walls_combine        = False\n",
    "voxelGrid_make_walls_combine_method = expNWalls\n",
    "voxelGrid_make_vectors_combine      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridPaths_hits_npy[0]: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grid_hits_0_80x80x80_withWalls_noErrors.npy\n",
      "gridPaths_primary_npy[0]: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grid_primary_0_80x80x80.npy\n",
      "gridPaths_h5[0]: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_0_80x80x80_withWalls_noErrors.h5\n"
     ]
    }
   ],
   "source": [
    "gridPaths_hits_npy    = [f\"{saveGrid_hits_basePath_npy}_{i}_{gridSize_input[0]}x{gridSize_input[1]}x{gridSize_input[2]}_{'withWalls' if voxelGrid_make_walls else 'noWalls'}_{'withErrors' if voxelGrid_make_errors else 'noErrors'}.npy\" for i in fileNumbers]\n",
    "gridPaths_primary_npy = [f\"{saveGrid_primary_basePath_npy}_{i}_{gridSize_output[0]}x{gridSize_output[1]}x{gridSize_output[2]}.npy\"                                                                                                        for i in fileNumbers]\n",
    "gridPaths_h5          = [f\"{saveGrids_h5}_{i}_{gridSize_output[0]}x{gridSize_output[1]}x{gridSize_output[2]}_{'withWalls' if voxelGrid_make_walls else 'noWalls'}_{'withErrors' if voxelGrid_make_errors else 'noErrors'}.h5\"             for i in fileNumbers]\n",
    "\n",
    "print('gridPaths_hits_npy[0]:', gridPaths_hits_npy[0])\n",
    "print('gridPaths_primary_npy[0]:', gridPaths_primary_npy[0])\n",
    "print('gridPaths_h5[0]:', gridPaths_h5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:01<00:00, 22.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def doGridCreationAndSave(dfPath_hits, dfPath_primary, gridPath_hits_npy, gridPath_primary_npy, gridPath_h5):\n",
    "    save_hits_npy    = True\n",
    "    save_primary_npy = True\n",
    "    save_h5          = True\n",
    "\n",
    "    if saveGridOnlyIfNoFile_hits_npy and (not saveGrid_hits_npy or os.path.exists(gridPath_hits_npy)):\n",
    "        save_hits_npy = False\n",
    "    if saveGridOnlyIfNoFile_primary_npy and (not saveGrid_primary_npy or os.path.exists(gridPath_primary_npy)):\n",
    "        save_primary_npy = False\n",
    "    if saveGridOnlyIfNoFile_h5 and (not saveGrids_h5 or os.path.exists(gridPath_h5)):\n",
    "        save_h5 = False\n",
    "\n",
    "    if not save_hits_npy and not save_primary_npy and not save_h5:\n",
    "        return\n",
    "    \n",
    "    x = None\n",
    "    y = None\n",
    "\n",
    "    if save_hits_npy or save_h5:\n",
    "        try:\n",
    "            df_hits    = pd.read_parquet(dfPath_hits)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            x = get_voxelGrid(\n",
    "                grid_shape           = np.array(gridSize_input).astype(int),\n",
    "                grid_dimensions      = np.array(DETECTOR_SIZE_MM),\n",
    "                vector_starts        = np.array(df_hits['sensor_position'].to_list()).reshape(-1, 3),\n",
    "                vector_directions    =-np.array(df_hits['reconstructedVector_direction'].to_list()).reshape(-1, 3),\n",
    "                vector_weights       = None,\n",
    "                vector_start_walls   = wallStringToInt(df_hits['sensor_wall'].to_numpy().astype(str)),\n",
    "                walls                = voxelGrid_make_walls,\n",
    "                walls_combine        = voxelGrid_make_walls_combine,\n",
    "                walls_combine_method = voxelGrid_make_walls_combine_method,\n",
    "                vector_combine       = voxelGrid_make_vectors_combine,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print('Error in processHitsFile:', e)\n",
    "        \n",
    "        assert x.max() <= len(df_hits), f\"Error: x.max() = {x.max()} > len(df_hits) = {len(df_hits)}\"\n",
    "        assert x.max() <= len(df_hits)-1, f\"Error: x.max() = {x.max()} > len(df_hits)-1 = {len(df_hits)-1}\"\n",
    "\n",
    "    if save_primary_npy or save_h5:\n",
    "        try:\n",
    "            df_primary = pd.read_parquet(dfPath_primary)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            y = make_voxelGrid_truth(\n",
    "                np.array(df_primary['position'].to_list()),\n",
    "                shape=gridSize_output,\n",
    "                detectorDimensions=DETECTOR_SIZE_MM,\n",
    "                makeErrors=False\n",
    "            )[0]\n",
    "        except Exception as e:\n",
    "            print('Error in processPrimaryFile:', e)\n",
    "\n",
    "        assert y.max() <= len(df_primary), f\"Error: y.max() = {y.max()} > len(df_primary) = {len(df_primary)}\"\n",
    "        assert y.max() <= len(df_primary)-1, f\"Error: y.max() = {y.max()} > len(df_primary)-1 = {len(df_primary)-1}\"\n",
    "\n",
    "    if save_h5:\n",
    "        with h5py.File(gridPath_h5, 'w') as f:\n",
    "            f.create_dataset('x', data=np.moveaxis(x, -1, 0))\n",
    "            f.create_dataset('y', data=y)\n",
    "    if save_hits_npy:\n",
    "        np.save(gridPath_hits_npy, x)\n",
    "    if save_primary_npy:\n",
    "        np.save(gridPath_primary_npy, y)\n",
    "\n",
    "args_list = [\n",
    "    (dfPaths_hits[i], dfPaths_primary[i], gridPaths_hits_npy[i], gridPaths_primary_npy[i], gridPaths_h5[i])\n",
    "    for i in range(len(dfPaths_hits))\n",
    "]\n",
    "\n",
    "if saveGrid_hits_npy or saveGrid_primary_npy or saveGrids_h5:\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(doGridCreationAndSave)(*args) for args in tqdm.tqdm(args_list)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paths_train): 5\n",
      "len(paths_test): 20\n",
      "len(paths_val): 10\n"
     ]
    }
   ],
   "source": [
    "nTest = 20\n",
    "nVal  = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "testIndices = np.random.choice(len(dfPaths_hits), nTest, replace=False)\n",
    "valIndices  = np.random.choice(np.delete(np.arange(len(dfPaths_hits)), testIndices), nVal, replace=False)\n",
    "\n",
    "X_train_indices = np.delete(np.arange(len(dfPaths_hits)), np.concatenate((testIndices, valIndices)))\n",
    "Y_train_indices = np.delete(np.arange(len(dfPaths_hits)), np.concatenate((testIndices, valIndices)))\n",
    "X_test_indices  = np.array(testIndices)\n",
    "Y_test_indices  = np.array(testIndices)\n",
    "X_val_indices   = np.array(valIndices)\n",
    "Y_val_indices   = np.array(valIndices)\n",
    "\n",
    "paths_train = [gridPaths_h5[i] for i in X_train_indices]\n",
    "paths_test  = [gridPaths_h5[i] for i in X_test_indices]\n",
    "paths_val   = [gridPaths_h5[i] for i in X_val_indices]\n",
    "\n",
    "# print('X_train_indices:', X_train_indices.tolist())\n",
    "# print('X_test_indices:' , X_test_indices .tolist())\n",
    "# print('X_val_indices:'  , X_val_indices  .tolist())\n",
    "\n",
    "print('len(paths_train):', len(paths_train))\n",
    "print('len(paths_test):' , len(paths_test))\n",
    "print('len(paths_val):'  , len(paths_val))\n",
    "\n",
    "assert len(paths_train) == len(X_train_indices)\n",
    "assert len(paths_test ) == len(X_test_indices )\n",
    "assert len(paths_val  ) == len(X_val_indices  )\n",
    "\n",
    "assert len(np.intersect1d(X_train_indices, X_test_indices)) == 0, f\"Overlap: {np.intersect1d(X_train_indices, X_test_indices)}\"\n",
    "assert len(np.intersect1d(X_train_indices, X_val_indices )) == 0, f\"Overlap: {np.intersect1d(X_train_indices, X_val_indices )}\"\n",
    "assert len(np.intersect1d(X_test_indices , X_val_indices )) == 0, f\"Overlap: {np.intersect1d(X_test_indices , X_val_indices )}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = [] #[1, 2, 3]\n",
    "\n",
    "for ind in indicies:\n",
    "    with h5py.File(gridPaths_h5[ind], 'r') as f:\n",
    "        x = f['x'][:]\n",
    "\n",
    "    nHits = x[ind].sum(axis=-1)\n",
    "    nHits = nHits.flatten()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "    n, _, _ = ax.hist(nHits, bins=np.arange(nHits.min(), nHits.max(), (nHits.max()-nHits.min())/100), histtype='step', color='black')\n",
    "    ax.vlines(np.mean(nHits), 0, np.max(n), color='red')\n",
    "    ax.vlines(np.mean(nHits)+np.std(nHits), 0, np.max(n), color='blue')\n",
    "    ax.vlines(np.mean(nHits)-np.std(nHits), 0, np.max(n), color='blue')\n",
    "\n",
    "    # ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.set_xlabel('Number of hits')\n",
    "    ax.set_ylabel('Number of voxels')\n",
    "    ax.set_title('Number of hits per voxel')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 87992.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib qt\n",
    "%matplotlib inline\n",
    "\n",
    "indices = [1,2,3]#[46, 51, 52]\n",
    "# indices = X_train_indices\n",
    "\n",
    "plots = [False, False, True]\n",
    "# plots = [True, True, True]\n",
    "format = 'pdf'\n",
    "resolution = 300\n",
    "\n",
    "xEdges = np.linspace(-DETECTOR_SIZE_MM[0]/2, DETECTOR_SIZE_MM[0]/2, gridSize_input[0] + 1)\n",
    "yEdges = np.linspace(-DETECTOR_SIZE_MM[1]/2, DETECTOR_SIZE_MM[1]/2, gridSize_input[1] + 1)\n",
    "zEdges = np.linspace(-DETECTOR_SIZE_MM[2]/2, DETECTOR_SIZE_MM[2]/2, gridSize_input[2] + 1)\n",
    "\n",
    "yEdges, xEdges, zEdges = np.meshgrid(xEdges, yEdges, zEdges)\n",
    "\n",
    "alpha_empty  = 0.10\n",
    "alpha_filled = 0.45\n",
    "alpha_true   = 0.70\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "for ind in tqdm.tqdm(indices):\n",
    "    continue\n",
    "\n",
    "    x = np.load(gridPaths_hits_npy[ind])\n",
    "    y = np.load(gridPaths_primary_npy[ind])\n",
    "\n",
    "    vals = x.sum(axis=-1)\n",
    "\n",
    "    # set minHits to the 1 sigma value\n",
    "    valsMinusOuterLayor = vals[1:-1, 1:-1, 1:-1]\n",
    "    minHits = valsMinusOuterLayor.mean()\n",
    "    maxHits = valsMinusOuterLayor.max()\n",
    "\n",
    "    # globalColorNorm = cm.colors.Normalize(vmin=0, vmax=np.max([X_train[ind].sum(axis=-1).max()]))\n",
    "    globalColorNorm = cm.colors.Normalize(vmin=minHits, vmax=maxHits)\n",
    "    # globalColorNorm = cm.colors.LogNorm(vmin=1, vmax=25)\n",
    "\n",
    "    vals = np.where(vals < minHits, 0, vals)\n",
    "    colors = cm.viridis(globalColorNorm(vals))\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "\n",
    "    if plots[0]:\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        ax = plot_grid(\n",
    "            ax,\n",
    "            xEdges,\n",
    "            yEdges,\n",
    "            zEdges,\n",
    "            recoGrid=vals,\n",
    "            recoGridFaceColors=colors,\n",
    "            recoGridEdgeColors=np.clip(colors*2-0.5, 0, 1),\n",
    "            recoGridAlpha=alpha_filled,\n",
    "            linewidth=0.5,\n",
    "            cbar=True,\n",
    "            colorNorm=globalColorNorm,\n",
    "            cmap=cm.viridis\n",
    "        )\n",
    "\n",
    "        plt.savefig('../figures/voxelGrid_{}.{}'.format(ind, format), bbox_inches='tight', transparent=True, dpi=resolution)\n",
    "        plt.show()\n",
    "\n",
    "    ##################################################################################################\n",
    "    ##################################################################################################\n",
    "    ##################################################################################################\n",
    "\n",
    "    if plots[1]:\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        primary = pd.read_parquet(dfPaths_primary[X_train_indices[ind]])['position'].to_list()\n",
    "        primary = np.array(primary)\n",
    "\n",
    "        ax = plot_grid(\n",
    "            ax,\n",
    "            xEdges,\n",
    "            yEdges,\n",
    "            zEdges,\n",
    "            trueGrid=y,\n",
    "            truePoints=primary,\n",
    "            trueGridFaceColors='r',\n",
    "            trueGridEdgeColors='k',\n",
    "            trueGridAlpha=alpha_true,\n",
    "            linewidth=0.5,\n",
    "            truePointsColor='b', \n",
    "            truePointsSize=10, \n",
    "            truePointsAlpha=1, \n",
    "            truePointsMarker='o'\n",
    "        )\n",
    "\n",
    "        plt.savefig('../figures/voxelGrid_truth_{}.{}'.format(ind, format), bbox_inches='tight', transparent=True, dpi=resolution)\n",
    "        plt.show()\n",
    "\n",
    "    ##################################################################################################\n",
    "    ##################################################################################################\n",
    "    ##################################################################################################\n",
    "    ##################################################################################################\n",
    "\n",
    "    if plots[2]:\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        ax = plot_grid(\n",
    "            ax,\n",
    "            xEdges,\n",
    "            yEdges,\n",
    "            zEdges,\n",
    "            recoGrid=vals,\n",
    "            recoGridFaceColors=colors,\n",
    "            recoGridEdgeColors=np.clip(colors*2-0.5, 0, 1),\n",
    "            recoGridAlpha=alpha_filled,\n",
    "            trueGrid=y,\n",
    "            trueGridFaceColors='r',\n",
    "            trueGridEdgeColors='k',\n",
    "            trueGridAlpha=alpha_true,\n",
    "            linewidth=0.5,\n",
    "            cbar=True,\n",
    "            colorNorm=globalColorNorm,\n",
    "            cmap=cm.viridis,\n",
    "            ignoreOuterNVoxels=0\n",
    "        )\n",
    "\n",
    "        ax.set_axis_off()\n",
    "        ax.set_aspect('equal')\n",
    "        plt.savefig('../figures/voxelGrid_both_{}.{}'.format(ind, format), bbox_inches='tight', transparent=True, dpi=resolution)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-05 16:15:35,672 [MainThread] WARNING ConfigLoader - CUDA not available, using CPU\n",
      "2024-11-05 16:15:35,672 [MainThread] INFO ConfigLoader - Using MPS\n",
      "2024-11-05 16:15:35,673 [MainThread] INFO TrainingSetup - {'device': 'mps', 'eval_metric': {'name': 'BoundaryAdaptedRandError', 'threshold': 0.4, 'use_first_input': True, 'use_last_target': True}, 'loaders': {'label_internal_path': '/y', 'num_workers': 10, 'raw_internal_path': '/x', 'train': {'file_paths': ['/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_7_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_20_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_30_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_32_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_34_80x80x80_withWalls_noErrors.h5'], 'slice_builder': {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}, 'transformer': {'label': [{'name': 'RandomFlip'}, {'name': 'RandomRotate90'}, {'angle_spectrum': 45, 'axes': [[2, 1]], 'mode': 'reflect', 'name': 'RandomRotate'}, {'name': 'ElasticDeformation', 'spline_order': 0}, {'append_label': True, 'name': 'StandardLabelToBoundary'}, {'expand_dims': False, 'name': 'ToTensor'}], 'raw': [{'name': 'Standardize'}, {'name': 'RandomFlip'}, {'name': 'RandomRotate90'}, {'angle_spectrum': 45, 'axes': [[2, 1]], 'mode': 'reflect', 'name': 'RandomRotate'}, {'name': 'ElasticDeformation', 'spline_order': 3}, {'execution_probability': 0.5, 'name': 'GaussianBlur3D'}, {'execution_probability': 0.2, 'name': 'AdditiveGaussianNoise'}, {'execution_probability': 0.2, 'name': 'AdditivePoissonNoise'}, {'expand_dims': True, 'name': 'ToTensor'}]}}, 'val': {'file_paths': ['/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_25_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_6_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_18_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_3_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_28_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_22_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_14_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_31_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_23_80x80x80_withWalls_noErrors.h5', '/Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_10_80x80x80_withWalls_noErrors.h5'], 'slice_builder': {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}, 'transformer': {'label': [{'append_label': True, 'name': 'StandardLabelToBoundary'}, {'expand_dims': False, 'name': 'ToTensor'}], 'raw': [{'name': 'Standardize'}, {'expand_dims': True, 'name': 'ToTensor'}]}}}, 'loss': {'ignore_index': None, 'name': 'BCEDiceLoss', 'skip_last_target': True}, 'lr_scheduler': {'factor': 0.5, 'mode': 'min', 'name': 'ReduceLROnPlateau', 'patience': 30}, 'model': {'f_maps': 32, 'final_sigmoid': True, 'in_channels': 6, 'layer_order': 'crg', 'name': 'UNet3D', 'num_groups': 8, 'out_channels': 1}, 'optimizer': {'learning_rate': 0.0002, 'weight_decay': 1e-05}, 'trainer': {'checkpoint_dir': '/Users/noah-everett/Documents/FNAL/Geant4/USSD_Geant4/analysis/data/h5-UNet', 'eval_score_higher_is_better': False, 'log_after_iters': 500, 'max_num_epochs': 1000, 'max_num_iterations': 150000, 'pre_trained': None, 'resume': None, 'validate_after_iters': 1000}}\n",
      "2024-11-05 16:15:35,688 [MainThread] INFO UNetTrainer - Using MPS\n",
      "2024-11-05 16:15:35,705 [MainThread] INFO UNetTrainer - Number of learnable params 4083041\n",
      "2024-11-05 16:15:35,705 [MainThread] INFO Dataset - Creating training and validation set loaders...\n",
      "2024-11-05 16:15:35,706 [MainThread] WARNING Dataset - Cannot find dataset class in the config. Using default 'StandardHDF5Dataset'.\n",
      "2024-11-05 16:15:35,778 [MainThread] INFO HDF5Dataset - Loading train set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_7_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,781 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,789 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,790 [MainThread] INFO HDF5Dataset - Loading train set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_20_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,793 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,794 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,795 [MainThread] INFO HDF5Dataset - Loading train set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_30_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,798 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,798 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,799 [MainThread] INFO HDF5Dataset - Loading train set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_32_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,803 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,803 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,804 [MainThread] INFO HDF5Dataset - Loading train set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_34_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,807 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,807 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,808 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_25_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,810 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,810 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,811 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_6_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,812 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,813 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,813 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_18_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,814 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,815 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,815 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_3_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,817 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,818 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,818 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_28_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,821 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,822 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,822 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_22_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,825 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,825 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,826 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_14_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,828 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,830 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,830 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_31_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,832 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,833 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,834 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_23_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,836 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,836 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,837 [MainThread] INFO HDF5Dataset - Loading val set from: /Users/noah-everett/Documents_non-iCloud/DSPS/tmp/MLdata/MLdata_grids_10_80x80x80_withWalls_noErrors.h5...\n",
      "2024-11-05 16:15:35,839 [MainThread] INFO Dataset - Slice builder config: {'name': 'SliceBuilder', 'patch_shape': [40, 40, 40], 'skip_shape_check': True, 'slack_acceptance': 0.01, 'stride_shape': [40, 40, 40], 'threshold': 0.6}\n",
      "2024-11-05 16:15:35,840 [MainThread] INFO HDF5Dataset - Number of patches: 8\n",
      "2024-11-05 16:15:35,840 [MainThread] INFO Dataset - Number of workers for train/val dataloader: 10\n",
      "2024-11-05 16:15:35,841 [MainThread] INFO Dataset - Batch size for train/val loader: 1\n",
      "2024-11-05 16:15:36,013 [MainThread] INFO UNetTrainer - UNet3D(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(6, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Encoder(\n",
      "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): Decoder(\n",
      "      (upsampling): InterpolateUpsampling()\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Decoder(\n",
      "      (upsampling): InterpolateUpsampling()\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Decoder(\n",
      "      (upsampling): InterpolateUpsampling()\n",
      "      (basic_module): DoubleConv(\n",
      "        (SingleConv1): SingleConv(\n",
      "          (conv): Conv3d(96, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (SingleConv2): SingleConv(\n",
      "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "          (ReLU): ReLU(inplace=True)\n",
      "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  (final_activation): Sigmoid()\n",
      ")\n",
      "2024-11-05 16:15:36,014 [MainThread] INFO UNetTrainer - eval_score_higher_is_better: False\n",
      "2024-11-05 16:15:40,736 [MainThread] INFO UNetTrainer - Training iteration [1/150000]. Epoch [0/999]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::slow_conv3d_forward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# config = get_config(tmpDir+'/train/', tmpDir+'/val/', num_workers=10)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m save_config(config, tmpDir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/config.yml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m main([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--config\u001b[39m\u001b[38;5;124m'\u001b[39m, tmpDir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/config.yml\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/pytorch3dunet-1.8.7-py3.11.egg/pytorch3dunet/train.py:31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(textArgs)\u001b[0m\n\u001b[1;32m     29\u001b[0m copy_config(config, config_path)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/pytorch3dunet-1.8.7-py3.11.egg/pytorch3dunet/unet3d/trainer.py:156\u001b[0m, in \u001b[0;36mUNetTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_epochs):\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m         should_terminate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m should_terminate:\n\u001b[1;32m    159\u001b[0m             logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopping criterion is satisfied. Finishing training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/pytorch3dunet-1.8.7-py3.11.egg/pytorch3dunet/unet3d/trainer.py:183\u001b[0m, in \u001b[0;36mUNetTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining iteration [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_num_epochs\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28minput\u001b[39m, target, weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_training_batch(t)\n\u001b[0;32m--> 183\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pass(\u001b[38;5;28minput\u001b[39m, target, weight)\n\u001b[1;32m    185\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# compute gradients and update parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/pytorch3dunet-1.8.7-py3.11.egg/pytorch3dunet/unet3d/trainer.py:317\u001b[0m, in \u001b[0;36mUNetTrainer._forward_pass\u001b[0;34m(self, input, target, weight)\u001b[0m\n\u001b[1;32m    314\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/pytorch3dunet-1.8.7-py3.11.egg/pytorch3dunet/unet3d/model.py:88\u001b[0m, in \u001b[0;36mAbstractUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m encoders_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders:\n\u001b[0;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m encoder(x)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# reverse the encoder outputs to be aligned with the decoder\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     encoders_features\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/pytorch3dunet-1.8.7-py3.11.egg/pytorch3dunet/unet3d/buildingblocks.py:308\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling(x)\n\u001b[0;32m--> 308\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_module(x)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/Python_3_11_5/lib/python3.11/site-packages/torch/nn/modules/conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    607\u001b[0m )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::slow_conv3d_forward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "sys.path.append('../pytorch-3dunet/')\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "from pytorch3dunet.train import main\n",
    "\n",
    "config = get_config(paths_train, paths_val, num_workers=10)\n",
    "# config = get_config(tmpDir+'/train/', tmpDir+'/val/', num_workers=10)\n",
    "save_config(config, tmpDir+'/config.yml')\n",
    "\n",
    "main(['--config', tmpDir+'/config.yml'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
