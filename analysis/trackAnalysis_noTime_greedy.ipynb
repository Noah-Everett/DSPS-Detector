{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cd03f2",
   "metadata": {},
   "source": [
    "\n",
    "# Photon-Track Reconstruction by Iterative Voxel Peeling\n",
    "\n",
    "This notebook builds voxel grids from ROOT **histograms** and reconstructs a track by\n",
    "iteratively **peeling** photons using a greedy strategy. We support both seeding modes\n",
    "(**max voxel** / **average of top-K voxels**) and both growth modes (**connected neighbors**\n",
    "vs **global max**). Removal uses a physically motivated fraction per photon: either **uniform**\n",
    "(1 / number of voxels traversed) or **length-weighted** (voxel path length / total path length).\n",
    "\n",
    "**Key ideas:**\n",
    "\n",
    "- Build a sparse **photon↔voxel incidence** using a 3D DDA through the detector AABB.\n",
    "- The initial grid is just the sum over photons of those per-voxel contributions.\n",
    "- At each step, choose a voxel (seed or next), **reduce** the weights of all photons that pass\n",
    "  through it by their share attributable to that voxel, and **update** the grid **incrementally**\n",
    "  without recomputing from scratch.\n",
    "- Repeat until stopping criteria are met (min support, remaining weight, max steps, etc.).\n",
    "\n",
    "> **Note**: This notebook **re-implements the grid-building** logic in-place to facilitate\n",
    "> many iterations (one per selected voxel). We still **reuse your repo helpers** to read the\n",
    "> histogram hits and primary track points. You can modify and iterate quickly here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Environment & imports ----------------------------------------------------\n",
    "import os, sys, math, gc, logging, itertools, time\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import uproot\n",
    "\n",
    "# Allow imports from your repo (../python)\n",
    "sys.path.append('../python')\n",
    "\n",
    "# Domain helpers from your codebase\n",
    "from constants import CM_PER_RAD, MM_PER_CM, Y_LIM, DETECTOR_SIZE_MM\n",
    "from importMethods import (\n",
    "    get_histogram_hits_tuple,\n",
    "    get_histogram_nHits_total,\n",
    "    get_primary_position,\n",
    "    get_primary_pdg\n",
    ")\n",
    "from hitAccuracyMethods import (\n",
    "    make_r, make_theta, make_phi, make_reconstructedVector_direction, make_relativeVector\n",
    ")\n",
    "from filterMethods import filter_r\n",
    "\n",
    "# Optional plotting helpers from your repo\n",
    "from plotMethods import *\n",
    "import plotParameters\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "LOGGER = logging.getLogger('peel_reco')\n",
    "\n",
    "# Constants / utilities reused from script\n",
    "def r_to_theta(r):\n",
    "    \"\"\"Convert radius to theta using your constants.\"\"\"\n",
    "    return r / (CM_PER_RAD * MM_PER_CM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0eaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- User settings (tweak here) -----------------------------------------------\n",
    "# IO\n",
    "INPUT_DIR = '/path/to/your/root/files'   # Folder containing *.root files\n",
    "FILE_LIMIT = None                        # e.g., 10 to test; or None for all\n",
    "HIST_DIR = 'photoSensor_hits_histograms' # We use histogram-based hits\n",
    "PRIMARY_TREE = 'primary;1'               # For optional truth visualization\n",
    "\n",
    "# Grid\n",
    "GRID_SIZE = (80, 80, 80)                 # (nx, ny, nz)\n",
    "DETECTOR_MM = tuple(DETECTOR_SIZE_MM)    # (Lx, Ly, Lz) from your constants\n",
    "\n",
    "# Cuts\n",
    "APPLY_CUTS = True                        # Option to skip cuts\n",
    "MIN_N_HITS = 0\n",
    "MIN_PRIMARY_STEPS = 30\n",
    "PRIMARY_PDG = 13\n",
    "\n",
    "# Photon share / peeling\n",
    "ALPHA_MODE = 'length'  # 'uniform' or 'length' (length-weighted is usually better)\n",
    "ALPHA_SCALE = 1.0      # Scale for how much to remove per selected voxel event\n",
    "\n",
    "# Seeding & growth\n",
    "SEED_MODE = 'max'      # 'max' or 'avg' (average of top-K voxels)\n",
    "SEED_TOP_K = 50        # Only used for SEED_MODE='avg'\n",
    "GROW_MODE = 'connected'  # 'connected' or 'global' (next global max)\n",
    "CONNECTIVITY = 26        # 6, 18, or 26 neighbors for 'connected'\n",
    "ALLOW_RESEED_IF_STUCK = True\n",
    "\n",
    "# Stopping\n",
    "MIN_SUPPORT = 3.0          # Minimum grid value for a voxel to be considered\n",
    "MAX_STEPS = 500            # Limit of selected voxels\n",
    "REMAINING_WEIGHT_FRAC = 0.05  # Stop if total residual photon weight < 5%\n",
    "\n",
    "# Snapshots (grid at iterations) -- heavy! Use carefully.\n",
    "SAVE_SNAPSHOTS = False\n",
    "SNAPSHOT_EVERY = 1  # save every N steps (ignored if SAVE_SNAPSHOTS=False)\n",
    "\n",
    "# Plotting (downsample to speed up in-notebook 3D plots)\n",
    "RESHAPE_SIZE = 40      # e.g., 40 => average blocks to 40^3 for plotting\n",
    "PLOT_MIN_VAL = 400     # min value threshold for rendering (tune per dataset)\n",
    "\n",
    "# Misc\n",
    "NUM_WORKERS = 1        # Notebook-friendly; set >1 to parallelize where applicable\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Output directory for any artifacts you may later want to persist\n",
    "OUTPUT_DIR = './peel_outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65348c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Discover ROOT files & (optionally) apply cuts ----------------------------\n",
    "def discover_root_files(input_dir: str, file_limit: Optional[int]=None):\n",
    "    files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.root')])\n",
    "    if file_limit is not None:\n",
    "        files = files[:file_limit]\n",
    "    return files\n",
    "\n",
    "def passes_cuts(path: str) -> bool:\n",
    "    if not APPLY_CUTS:\n",
    "        return True\n",
    "    try:\n",
    "        n_hits = get_histogram_nHits_total(path, directoryName=HIST_DIR)\n",
    "        prim_steps = len(get_primary_position(path, PRIMARY_TREE))\n",
    "        if n_hits >= MIN_N_HITS and prim_steps >= MIN_PRIMARY_STEPS:\n",
    "            return True\n",
    "        else:\n",
    "            LOGGER.info(f\"Cut {os.path.basename(path)}: hits={n_hits}, steps={prim_steps}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(f\"Error applying cuts to {path}: {e}\")\n",
    "        return False\n",
    "\n",
    "all_files = discover_root_files(INPUT_DIR, FILE_LIMIT)\n",
    "files = [p for p in all_files if passes_cuts(p)]\n",
    "\n",
    "LOGGER.info(f\"Found {len(files)} ROOT files after cuts (from {len(all_files)} found).\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf0844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build hits DataFrame for one file (from histograms) ----------------------\n",
    "def build_hits_df_from_hist(path: str, hist_dir: str=HIST_DIR) -> pd.DataFrame:\n",
    "    # Using your helper to read histogram-based hits\n",
    "    ids, dirs, poss, walls, rb, rn = get_histogram_hits_tuple(path, hist_dir, True)\n",
    "    df = pd.DataFrame({\n",
    "        'sensor_name': ids,\n",
    "        'sensor_direction': dirs,\n",
    "        'sensor_position': poss,\n",
    "        'sensor_wall': walls,\n",
    "        'relativePosition_binned': rb,\n",
    "        'relativePosition_nBin': rn,\n",
    "    })\n",
    "    # Reconstruct unit directions (following your pipeline)\n",
    "    df = make_r(df)\n",
    "    df = filter_r(df, Y_LIM)\n",
    "    df = make_theta(df, r_to_theta)\n",
    "    df = make_phi(df)\n",
    "    df = make_reconstructedVector_direction(df)\n",
    "    if 'initialPosition' in df.columns:\n",
    "        df = make_relativeVector(df)\n",
    "    return df\n",
    "\n",
    "def get_primary_positions(path: str) -> np.ndarray:\n",
    "    \"\"\"Optional truth points for visualization (returns Nx3).\"\"\"\n",
    "    try:\n",
    "        pos = get_primary_position(path, PRIMARY_TREE)\n",
    "        pdgs = get_primary_pdg(path, PRIMARY_TREE)\n",
    "        pos = np.asarray([p for p, q in zip(pos, pdgs) if q == PRIMARY_PDG], dtype=float)\n",
    "        # Keep only positions inside detector bounds\n",
    "        half = np.asarray(DETECTOR_MM) / 2.0\n",
    "        mask = np.all((pos > -half) & (pos < half), axis=1)\n",
    "        return pos[mask]\n",
    "    except Exception as e:\n",
    "        LOGGER.warning(f\"Primary load failed ({os.path.basename(path)}): {e}\")\n",
    "        return np.empty((0,3), dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Geometry: AABB intersection & 3D DDA traversal ---------------------------\n",
    "EPS = 1e-9\n",
    "\n",
    "def aabb_intersect(origin: np.ndarray, direction: np.ndarray,\n",
    "                   bounds_min: np.ndarray, bounds_max: np.ndarray) -> Optional[Tuple[float,float]]:\n",
    "    \"\"\"Ray (origin + t*dir) vs axis-aligned box [min, max]. Return (t_enter, t_exit) or None.\"\"\"\n",
    "    inv = np.where(np.abs(direction) < EPS, np.inf, 1.0 / direction)\n",
    "    t0s = (bounds_min - origin) * inv\n",
    "    t1s = (bounds_max - origin) * inv\n",
    "    tmin = np.maximum.reduce(np.minimum(t0s, t1s))\n",
    "    tmax = np.minimum.reduce(np.maximum(t0s, t1s))\n",
    "    if tmax < max(tmin, 0.0):\n",
    "        return None\n",
    "    return max(tmin, 0.0), tmax\n",
    "\n",
    "def world_to_voxel(point_mm: np.ndarray, grid_shape: Tuple[int,int,int],\n",
    "                   det_mm: Tuple[float,float,float]) -> np.ndarray:\n",
    "    \"\"\"Map mm coords to fractional voxel coords (not clipped).\"\"\"\n",
    "    det_mm = np.asarray(det_mm, dtype=float)\n",
    "    gshape = np.asarray(grid_shape, dtype=int)\n",
    "    vsize = det_mm / gshape\n",
    "    vmin = -det_mm / 2.0\n",
    "    return (point_mm - vmin) / vsize  # fractional index coords\n",
    "\n",
    "def voxel_center_ijk(ijk: Tuple[int,int,int], grid_shape, det_mm):\n",
    "    gshape = np.asarray(grid_shape, dtype=int)\n",
    "    det_mm = np.asarray(det_mm, dtype=float)\n",
    "    vsize = det_mm / gshape\n",
    "    vmin  = -det_mm / 2.0\n",
    "    ijk = np.asarray(ijk, dtype=float)\n",
    "    return vmin + (ijk + 0.5) * vsize\n",
    "\n",
    "def dda_traverse(origin_mm: np.ndarray, direction_mm: np.ndarray,\n",
    "                 grid_shape: Tuple[int,int,int], det_mm: Tuple[float,float,float],\n",
    "                 t_enter: float, t_exit: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    3D DDA through voxel grid. Assumes `direction_mm` is **unit length** (mm units).\n",
    "    Returns:\n",
    "      - flat voxel indices (int)\n",
    "      - lengths inside each voxel (float, in mm)\n",
    "    \"\"\"\n",
    "    # Map entry point to fractional voxel coords\n",
    "    gshape = np.asarray(grid_shape, dtype=int)\n",
    "    det_mm = np.asarray(det_mm, dtype=float)\n",
    "    vsize = det_mm / gshape\n",
    "    vmin  = -det_mm / 2.0\n",
    "\n",
    "    # Start at entry\n",
    "    p = origin_mm + direction_mm * t_enter  # point in mm\n",
    "    g = (p - vmin) / vsize                  # fractional voxel coords\n",
    "    ijk = np.floor(g - EPS).astype(int)     # voxel index (clip later)\n",
    "\n",
    "    # Handle boundary edge case\n",
    "    ijk = np.clip(ijk, 0, gshape - 1)\n",
    "\n",
    "    # Steps & deltas\n",
    "    step = np.sign(direction_mm).astype(int)\n",
    "    step[step == 0] = 0\n",
    "\n",
    "    # Compute first boundary tMax per axis\n",
    "    tMax = np.empty(3, dtype=float)\n",
    "    tDelta = np.empty(3, dtype=float)\n",
    "    for ax in range(3):\n",
    "        if abs(direction_mm[ax]) < EPS:\n",
    "            tMax[ax] = np.inf\n",
    "            tDelta[ax] = np.inf\n",
    "        else:\n",
    "            if direction_mm[ax] > 0:\n",
    "                next_boundary = vmin[ax] + (ijk[ax] + 1) * vsize[ax]\n",
    "            else:\n",
    "                next_boundary = vmin[ax] + ijk[ax] * vsize[ax]\n",
    "            tMax[ax] = (next_boundary - p[ax]) / direction_mm[ax]\n",
    "            tDelta[ax] = abs(vsize[ax] / direction_mm[ax])\n",
    "\n",
    "    # DDA loop\n",
    "    voxels = []\n",
    "    lengths = []\n",
    "    t = t_enter\n",
    "    while t < t_exit - EPS:\n",
    "        # Record current voxel and length to next boundary\n",
    "        t_next = min(tMax[0], tMax[1], tMax[2], t_exit)\n",
    "        dt = max(0.0, t_next - t)\n",
    "        if dt > 0:\n",
    "            flat = (ijk[0] * gshape[1] + ijk[1]) * gshape[2] + ijk[2]\n",
    "            voxels.append(flat)\n",
    "            lengths.append(dt)  # since |direction| == 1 mm, dt is path length in mm\n",
    "\n",
    "        # Step along the axis with smallest tMax\n",
    "        ax = int(np.argmin(tMax))\n",
    "        if tMax[ax] >= t_exit:\n",
    "            break\n",
    "\n",
    "        ijk[ax] += step[ax]\n",
    "        # Exit if outside grid\n",
    "        if (ijk[ax] < 0) or (ijk[ax] >= gshape[ax]):\n",
    "            break\n",
    "\n",
    "        t = tMax[ax]\n",
    "        tMax[ax] += tDelta[ax]\n",
    "\n",
    "    if not voxels:\n",
    "        return np.empty((0,), dtype=int), np.empty((0,), dtype=float)\n",
    "\n",
    "    return np.asarray(voxels, dtype=int), np.asarray(lengths, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build photon↔voxel incidence (sparse) and initial grid -------------------\n",
    "def build_incidence_from_hits(df_hits: pd.DataFrame,\n",
    "                              grid_shape: Tuple[int,int,int]=GRID_SIZE,\n",
    "                              det_mm: Tuple[float,float,float]=DETECTOR_MM,\n",
    "                              alpha_mode: str = ALPHA_MODE):\n",
    "    \"\"\"\n",
    "    From hits DataFrame (with 'sensor_position' and 'reconstructedVector_direction'),\n",
    "    build:\n",
    "      - initial grid (sum over photons of voxel contributions)\n",
    "      - photon->voxel CSR-like structure\n",
    "      - voxel->photon CSR-like structure\n",
    "      - per-photon residual weights (init = 1.0)\n",
    "    \"\"\"\n",
    "    starts = np.vstack(df_hits['sensor_position'].to_numpy())\n",
    "    dirs   = -np.vstack(df_hits['reconstructedVector_direction'].to_numpy())  # back into detector\n",
    "\n",
    "    # Normalize directions\n",
    "    norms = np.linalg.norm(dirs, axis=1, keepdims=True)\n",
    "    safe = norms[:,0] > 0\n",
    "    dirs[safe] /= norms[safe]\n",
    "\n",
    "    bounds_min = -np.asarray(det_mm) / 2.0\n",
    "    bounds_max =  np.asarray(det_mm) / 2.0\n",
    "\n",
    "    # CSR-like containers\n",
    "    photon_ptr = [0]\n",
    "    photon_vox = []\n",
    "    photon_frac = []  # per-voxel share for that photon (sum to 1 per photon)\n",
    "\n",
    "    # COO for voxel->photon (we will sort & CSR it later)\n",
    "    coo_vox = []\n",
    "    coo_photon = []\n",
    "    coo_frac = []\n",
    "\n",
    "    # Initial grid (flat)\n",
    "    nvox = int(grid_shape[0] * grid_shape[1] * grid_shape[2])\n",
    "    grid_flat = np.zeros(nvox, dtype=np.float64)\n",
    "\n",
    "    for p_idx in tqdm(range(len(starts)), desc='Tracing photons', leave=False):\n",
    "        o = starts[p_idx].astype(float)\n",
    "        d = dirs[p_idx].astype(float)\n",
    "        inter = aabb_intersect(o, d, bounds_min, bounds_max)\n",
    "        if inter is None:\n",
    "            photon_ptr.append(photon_ptr[-1])\n",
    "            continue\n",
    "        t_enter, t_exit = inter\n",
    "        vox_idx, lengths = dda_traverse(o, d, grid_shape, det_mm, t_enter, t_exit)\n",
    "        if vox_idx.size == 0:\n",
    "            photon_ptr.append(photon_ptr[-1])\n",
    "            continue\n",
    "\n",
    "        if alpha_mode == 'uniform':\n",
    "            contrib = np.full_like(lengths, 1.0/len(lengths), dtype=np.float64)\n",
    "        else:  # 'length'\n",
    "            total_len = np.sum(lengths)\n",
    "            if total_len <= 0:\n",
    "                photon_ptr.append(photon_ptr[-1])\n",
    "                continue\n",
    "            contrib = lengths / total_len\n",
    "\n",
    "        # Append CSR segments\n",
    "        photon_vox.extend(vox_idx.tolist())\n",
    "        photon_frac.extend(contrib.tolist())\n",
    "        photon_ptr.append(photon_ptr[-1] + len(vox_idx))\n",
    "\n",
    "        # Accumulate initial grid\n",
    "        grid_flat[vox_idx] += contrib\n",
    "\n",
    "        # Build COO for voxel->photon\n",
    "        coo_vox.extend(vox_idx.tolist())\n",
    "        coo_photon.extend([p_idx] * len(vox_idx))\n",
    "        coo_frac.extend(contrib.tolist())\n",
    "\n",
    "    photon_ptr = np.asarray(photon_ptr, dtype=np.int64)\n",
    "    photon_vox = np.asarray(photon_vox, dtype=np.int32)\n",
    "    photon_frac = np.asarray(photon_frac, dtype=np.float32)\n",
    "\n",
    "    # Build voxel->photon CSR by sorting COO by voxel index\n",
    "    if len(coo_vox) == 0:\n",
    "        voxel_ptr = np.zeros(nvox+1, dtype=np.int64)\n",
    "        voxel_photon = np.empty((0,), dtype=np.int32)\n",
    "        voxel_frac = np.empty((0,), dtype=np.float32)\n",
    "    else:\n",
    "        coo_vox = np.asarray(coo_vox, dtype=np.int32)\n",
    "        order = np.argsort(coo_vox, kind='mergesort')  # stable\n",
    "        coo_vox = coo_vox[order]\n",
    "        voxel_photon = np.asarray(coo_photon, dtype=np.int32)[order]\n",
    "        voxel_frac = np.asarray(coo_frac, dtype=np.float32)[order]\n",
    "\n",
    "        voxel_ptr = np.zeros(nvox+1, dtype=np.int64)\n",
    "        # Count entries per voxel\n",
    "        np.add.at(voxel_ptr, coo_vox + 1, 1)\n",
    "        voxel_ptr = np.cumsum(voxel_ptr)\n",
    "\n",
    "    grid = grid_flat.reshape(grid_shape)\n",
    "\n",
    "    # Residual photon weights (init to 1)\n",
    "    photon_w = np.ones(len(starts), dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'grid': grid,\n",
    "        'photon_ptr': photon_ptr,\n",
    "        'photon_vox': photon_vox,\n",
    "        'photon_frac': photon_frac,\n",
    "        'voxel_ptr': voxel_ptr,\n",
    "        'voxel_photon': voxel_photon,\n",
    "        'voxel_frac': voxel_frac,\n",
    "        'photon_w': photon_w,\n",
    "        'grid_shape': tuple(grid_shape),\n",
    "        'det_mm': tuple(det_mm)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a03137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Utilities for seeding, neighbors, and peeling updates --------------------\n",
    "def grid_argmax(grid: np.ndarray, visited: Optional[np.ndarray]=None, min_support: float=MIN_SUPPORT) -> Optional[int]:\n",
    "    flat = grid.ravel()\n",
    "    if visited is not None:\n",
    "        flat = flat.copy()\n",
    "        flat[visited.ravel()] = -np.inf\n",
    "    idx = int(np.argmax(flat))\n",
    "    if not np.isfinite(flat[idx]) or flat[idx] < min_support:\n",
    "        return None\n",
    "    return idx\n",
    "\n",
    "def grid_seed_avg(grid: np.ndarray, top_k: int=SEED_TOP_K, det_mm: Tuple[float,float,float]=DETECTOR_MM) -> Optional[int]:\n",
    "    \"\"\"Center-of-mass of top-K voxels (weighted), snapped to nearest voxel.\"\"\"\n",
    "    gshape = np.asarray(grid.shape, dtype=int)\n",
    "    flat = grid.ravel()\n",
    "    if flat.size == 0:\n",
    "        return None\n",
    "    k = min(len(flat), max(1, int(top_k)))\n",
    "    idxs = np.argpartition(flat, -k)[-k:]\n",
    "    vals = flat[idxs]\n",
    "    if np.all(vals <= 0):\n",
    "        return None\n",
    "    # Compute weighted CoM in voxel coordinates\n",
    "    ijk = np.vstack(np.unravel_index(idxs, grid.shape)).T.astype(float)\n",
    "    w = vals.astype(float)\n",
    "    com = np.average(ijk, weights=w, axis=0)\n",
    "    com = np.clip(np.round(com), 0, gshape - 1).astype(int)\n",
    "    return int((com[0] * gshape[1] + com[1]) * gshape[2] + com[2])\n",
    "\n",
    "def neighbors_of(flat_idx: int, grid_shape: Tuple[int,int,int], connectivity: int=CONNECTIVITY) -> List[int]:\n",
    "    nx, ny, nz = grid_shape\n",
    "    i = flat_idx // (ny * nz)\n",
    "    j = (flat_idx % (ny * nz)) // nz\n",
    "    k = flat_idx % nz\n",
    "\n",
    "    offsets = []\n",
    "    if connectivity == 6:\n",
    "        offsets = [(1,0,0),(-1,0,0),(0,1,0),(0,-1,0),(0,0,1),(0,0,-1)]\n",
    "    elif connectivity == 18:\n",
    "        for di in [-1,0,1]:\n",
    "            for dj in [-1,0,1]:\n",
    "                for dk in [-1,0,1]:\n",
    "                    if (di,dj,dk) == (0,0,0): continue\n",
    "                    if abs(di) + abs(dj) + abs(dk) <= 2:\n",
    "                        offsets.append((di,dj,dk))\n",
    "    else:  # 26\n",
    "        for di in [-1,0,1]:\n",
    "            for dj in [-1,0,1]:\n",
    "                for dk in [-1,0,1]:\n",
    "                    if (di,dj,dk) == (0,0,0): continue\n",
    "                    offsets.append((di,dj,dk))\n",
    "\n",
    "    nbs = []\n",
    "    for di,dj,dk in offsets:\n",
    "        ii, jj, kk = i+di, j+dj, k+dk\n",
    "        if 0 <= ii < nx and 0 <= jj < ny and 0 <= kk < nz:\n",
    "            nbs.append((ii * ny + jj) * nz + kk)\n",
    "    return nbs\n",
    "\n",
    "def peel_once(selected_voxel: int, state: Dict, alpha_scale: float=ALPHA_SCALE) -> float:\n",
    "    \"\"\"\n",
    "    Peel one voxel: reduce photon weights for photons traversing it by their fractional share,\n",
    "    then update the grid incrementally. Returns total weight removed (sum dw_p).\n",
    "    \"\"\"\n",
    "    grid = state['grid']\n",
    "    photon_ptr = state['photon_ptr']\n",
    "    photon_vox = state['photon_vox']\n",
    "    photon_frac = state['photon_frac']\n",
    "    voxel_ptr = state['voxel_ptr']\n",
    "    voxel_photon = state['voxel_photon']\n",
    "    voxel_frac = state['voxel_frac']\n",
    "    photon_w = state['photon_w']\n",
    "    gshape = state['grid_shape']\n",
    "\n",
    "    # Photons hitting this voxel (CSR slice)\n",
    "    start = voxel_ptr[selected_voxel]\n",
    "    end   = voxel_ptr[selected_voxel + 1]\n",
    "    if end <= start:\n",
    "        return 0.0\n",
    "\n",
    "    p_idx_arr = voxel_photon[start:end]\n",
    "    frac_vp   = voxel_frac[start:end]  # per-photon share for this voxel\n",
    "    # Compute weight reductions (cap at available weight)\n",
    "    dw = np.minimum(photon_w[p_idx_arr], alpha_scale * frac_vp * photon_w[p_idx_arr])\n",
    "    if np.all(dw <= 0):\n",
    "        return 0.0\n",
    "\n",
    "    # Apply on grid: for each affected photon p, subtract dw_p * contrib over all its voxels\n",
    "    for p_idx, dw_p in zip(p_idx_arr, dw):\n",
    "        if dw_p <= 0: \n",
    "            continue\n",
    "        # photon CSR segment\n",
    "        ps = photon_ptr[p_idx]\n",
    "        pe = photon_ptr[p_idx + 1]\n",
    "        if pe <= ps:\n",
    "            continue\n",
    "        vxs = photon_vox[ps:pe]\n",
    "        frc = photon_frac[ps:pe]\n",
    "        # grid -= dw_p * frc at these voxels\n",
    "        # operate on flat grid\n",
    "        grid.reshape(-1)[vxs] -= dw_p * frc\n",
    "        photon_w[p_idx] -= dw_p\n",
    "\n",
    "    # Clamp grid to >= 0 small epsilon to avoid negative drift\n",
    "    np.maximum(grid, 0.0, out=grid)\n",
    "\n",
    "    removed = float(np.sum(dw))\n",
    "    return removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Full peeling loop ---------------------------------------------------------\n",
    "def run_peeling(state: Dict,\n",
    "                seed_mode: str=SEED_MODE,\n",
    "                grow_mode: str=GROW_MODE,\n",
    "                connectivity: int=CONNECTIVITY,\n",
    "                min_support: float=MIN_SUPPORT,\n",
    "                max_steps: int=MAX_STEPS,\n",
    "                remaining_weight_frac: float=REMAINING_WEIGHT_FRAC,\n",
    "                allow_reseed: bool=ALLOW_RESEED_IF_STUCK,\n",
    "                save_snapshots: bool=SAVE_SNAPSHOTS,\n",
    "                snapshot_every: int=SNAPSHOT_EVERY):\n",
    "    grid = state['grid']\n",
    "    nvox = grid.size\n",
    "    visited = np.zeros(nvox, dtype=bool)\n",
    "    gshape = grid.shape\n",
    "\n",
    "    # Seed\n",
    "    if seed_mode == 'avg':\n",
    "        seed = grid_seed_avg(grid, top_k=SEED_TOP_K)\n",
    "    else:\n",
    "        seed = grid_argmax(grid, visited=None, min_support=min_support)\n",
    "\n",
    "    if seed is None:\n",
    "        LOGGER.warning('No valid seed found (grid below min_support).')\n",
    "        return {\n",
    "            'track_flat': [],\n",
    "            'snapshots': [],\n",
    "            'grid_final': grid,\n",
    "            'visited_mask': visited.reshape(gshape),\n",
    "        }\n",
    "\n",
    "    # Frontier for 'connected' growth\n",
    "    frontier = set()\n",
    "    track = []\n",
    "\n",
    "    def consider_neighbors(v):\n",
    "        for nb in neighbors_of(v, gshape, connectivity=connectivity):\n",
    "            if not visited[nb]:\n",
    "                frontier.add(nb)\n",
    "\n",
    "    # Initialize\n",
    "    current = seed\n",
    "    track.append(current)\n",
    "    visited[current] = True\n",
    "    consider_neighbors(current)\n",
    "\n",
    "    snapshots = []\n",
    "    if save_snapshots:\n",
    "        snapshots.append(grid.copy())\n",
    "\n",
    "    # Peel the seed\n",
    "    peel_once(current, state, alpha_scale=ALPHA_SCALE)\n",
    "\n",
    "    for step in range(1, max_steps):\n",
    "        # Stopping: remaining photon weight fraction\n",
    "        rem_frac = float(np.sum(state['photon_w'])) / len(state['photon_w'])\n",
    "        if rem_frac <= remaining_weight_frac:\n",
    "            LOGGER.info(f'Stopping at step {step}: remaining photon weight frac={rem_frac:.3f}')\n",
    "            break\n",
    "\n",
    "        # Choose next voxel\n",
    "        next_vox = None\n",
    "        if grow_mode == 'connected':\n",
    "            if frontier:\n",
    "                # pick frontier voxel with highest current support\n",
    "                frontier_list = list(frontier)\n",
    "                vals = grid.reshape(-1)[frontier_list]\n",
    "                # filter by min_support\n",
    "                vals_mask = vals >= min_support\n",
    "                if np.any(vals_mask):\n",
    "                    best_idx = int(np.argmax(vals * vals_mask))\n",
    "                    next_vox = frontier_list[best_idx]\n",
    "                else:\n",
    "                    frontier.clear()\n",
    "            if next_vox is None and allow_reseed:\n",
    "                # reseed globally if stuck\n",
    "                next_vox = grid_argmax(grid, visited=visited.reshape(gshape), min_support=min_support)\n",
    "        else:  # 'global'\n",
    "            next_vox = grid_argmax(grid, visited=visited.reshape(gshape), min_support=min_support)\n",
    "\n",
    "        if next_vox is None:\n",
    "            LOGGER.info(f'Stopping at step {step}: no voxel meets min_support.')\n",
    "            break\n",
    "\n",
    "        # Update track\n",
    "        track.append(next_vox)\n",
    "        visited[next_vox] = True\n",
    "        if grow_mode == 'connected':\n",
    "            consider_neighbors(next_vox)\n",
    "\n",
    "        # Peel\n",
    "        peel_once(next_vox, state, alpha_scale=ALPHA_SCALE)\n",
    "\n",
    "        # Snapshot\n",
    "        if save_snapshots and (step % max(1, snapshot_every) == 0):\n",
    "            snapshots.append(grid.copy())\n",
    "\n",
    "    return {\n",
    "        'track_flat': track,\n",
    "        'snapshots': snapshots,\n",
    "        'grid_final': grid,\n",
    "        'visited_mask': visited.reshape(gshape),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- End-to-end for one file ---------------------------------------------------\n",
    "def process_file(path: str,\n",
    "                 grid_size: Tuple[int,int,int]=GRID_SIZE,\n",
    "                 seed_mode: str=SEED_MODE,\n",
    "                 grow_mode: str=GROW_MODE):\n",
    "    LOGGER.info(f'Processing: {os.path.basename(path)}')\n",
    "    df_hits = build_hits_df_from_hist(path, HIST_DIR)\n",
    "    if len(df_hits) == 0:\n",
    "        LOGGER.warning('No hits after filtering.')\n",
    "        return None\n",
    "\n",
    "    state = build_incidence_from_hits(df_hits, grid_size, DETECTOR_MM, ALPHA_MODE)\n",
    "\n",
    "    # Optional truth\n",
    "    primary_pos = get_primary_positions(path)\n",
    "\n",
    "    # Run peeling\n",
    "    res = run_peeling(state,\n",
    "                      seed_mode=seed_mode,\n",
    "                      grow_mode=grow_mode,\n",
    "                      connectivity=CONNECTIVITY,\n",
    "                      min_support=MIN_SUPPORT,\n",
    "                      max_steps=MAX_STEPS,\n",
    "                      remaining_weight_frac=REMAINING_WEIGHT_FRAC,\n",
    "                      allow_reseed=ALLOW_RESEED_IF_STUCK,\n",
    "                      save_snapshots=SAVE_SNAPSHOTS,\n",
    "                      snapshot_every=SNAPSHOT_EVERY)\n",
    "\n",
    "    res.update({\n",
    "        'file': path,\n",
    "        'primary_pos': primary_pos,\n",
    "        'grid_shape': state['grid_shape'],\n",
    "        'det_mm': state['det_mm']\n",
    "    })\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Plotting helpers ----------------------------------------------------------\n",
    "def downsample_grid(arr: np.ndarray, reshape_size: int) -> np.ndarray:\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    RS = int(reshape_size)\n",
    "    step = (arr.shape[0]//RS, arr.shape[1]//RS, arr.shape[2]//RS)\n",
    "    assert (arr.shape[0] % RS == 0) and (arr.shape[1] % RS == 0) and (arr.shape[2] % RS == 0), \\'Grid not divisible by reshape size\\'\n",
    "    return arr.reshape(RS, step[0], RS, step[1], RS, step[2]).mean(axis=(1,3,5))\n",
    "\n",
    "def make_edges(det_mm: Tuple[float,float,float], grid_shape: Tuple[int,int,int]):\n",
    "    gx, gy, gz = grid_shape\n",
    "    xEdges = np.linspace(-det_mm[0]/2, det_mm[0]/2, gx + 1)\n",
    "    yEdges = np.linspace(-det_mm[1]/2, det_mm[1]/2, gy + 1)\n",
    "    zEdges = np.linspace(-det_mm[2]/2, det_mm[2]/2, gz + 1)\n",
    "    yEdges, xEdges, zEdges = np.meshgrid(xEdges, yEdges, zEdges)  # matching your example\n",
    "    return xEdges, yEdges, zEdges\n",
    "\n",
    "def plot_pred_true(pred_grid: np.ndarray, true_grid: Optional[np.ndarray],\n",
    "                   det_mm: Tuple[float,float,float], reshape_size: int=RESHAPE_SIZE,\n",
    "                   min_val: float=PLOT_MIN_VAL):\n",
    "    pred = downsample_grid(pred_grid, reshape_size)\n",
    "    true = downsample_grid(true_grid, reshape_size) if true_grid is not None else None\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    gridSize = pred.shape\n",
    "    xEdges, yEdges, zEdges = make_edges(det_mm, gridSize)\n",
    "\n",
    "    maxVal = np.max(pred) if pred.size>0 else 1.0\n",
    "    alpha_filled = 0.5\n",
    "    globalColorNorm = cm.colors.Normalize(vmin=min_val, vmax=maxVal)\n",
    "    pred_viz = np.where(pred < min_val, 0, pred)\n",
    "    colors = cm.viridis(globalColorNorm(pred_viz))\n",
    "\n",
    "    ax = plot_grid(\n",
    "        ax,\n",
    "        xEdges,\n",
    "        yEdges,\n",
    "        zEdges,\n",
    "        recoGrid=pred_viz,\n",
    "        recoGridFaceColors=colors,\n",
    "        recoGridEdgeColors=np.clip(colors*2-0.5, 0, 1),\n",
    "        recoGridAlpha=alpha_filled,\n",
    "        trueGrid=true if true is not None else None,\n",
    "        trueGridEdgeColors='red',\n",
    "        trueGridAlpha=0,\n",
    "        nullGridAlpha=0.3,\n",
    "        linewidth=0.5,\n",
    "        cbar=True,\n",
    "        colorNorm=globalColorNorm,\n",
    "        cmap=cm.viridis,\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build a simple truth grid from primary points (visual aid) ---------------\n",
    "def primary_points_to_grid(primary_pos: np.ndarray,\n",
    "                           grid_shape: Tuple[int,int,int],\n",
    "                           det_mm: Tuple[float,float,float]) -> np.ndarray:\n",
    "    \"\"\"Nearest-voxel stamping of primary positions; visualization-only.\"\"\"\n",
    "    grid = np.zeros(grid_shape, dtype=np.float32)\n",
    "    if primary_pos.size == 0:\n",
    "        return grid\n",
    "    det_mm = np.asarray(det_mm, dtype=float)\n",
    "    vmin = -det_mm / 2.0\n",
    "    vmax =  det_mm / 2.0\n",
    "    vsize = det_mm / np.asarray(grid_shape, dtype=int)\n",
    "    g = (primary_pos - vmin) / vsize\n",
    "    ijk = np.floor(g).astype(int)\n",
    "    ijk = np.clip(ijk, 0, np.asarray(grid_shape)-1)\n",
    "    flat = (ijk[:,0] * grid_shape[1] + ijk[:,1]) * grid_shape[2] + ijk[:,2]\n",
    "    vals = np.bincount(flat, minlength=grid.size).astype(np.float32)\n",
    "    return vals.reshape(grid_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79413dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Run for the first file (demo both strategies) ----------------------------\n",
    "if len(files) == 0:\n",
    "    LOGGER.warning('No files to process. Please set INPUT_DIR correctly.')\n",
    "else:\n",
    "    test_file = files[0]\n",
    "    LOGGER.info(f'Demo on: {os.path.basename(test_file)}')\n",
    "\n",
    "    # Variant A: seed=max, grow=connected\n",
    "    SEED_MODE_A = 'max'\n",
    "    GROW_MODE_A = 'connected'\n",
    "    resA = process_file(test_file, GRID_SIZE, seed_mode=SEED_MODE_A, grow_mode=GROW_MODE_A)\n",
    "\n",
    "    # Variant B: seed=avg, grow=global\n",
    "    SEED_MODE_B = 'avg'\n",
    "    GROW_MODE_B = 'global'\n",
    "    resB = process_file(test_file, GRID_SIZE, seed_mode=SEED_MODE_B, grow_mode=GROW_MODE_B)\n",
    "\n",
    "    # Visualization (pred vs optional truth)\n",
    "    if resA is not None:\n",
    "        true_gridA = primary_points_to_grid(resA['primary_pos'], resA['grid_shape'], resA['det_mm'])\n",
    "        plot_pred_true(resA['grid_final'], true_gridA, resA['det_mm'], reshape_size=RESHAPE_SIZE, min_val=PLOT_MIN_VAL)\n",
    "    if resB is not None:\n",
    "        true_gridB = primary_points_to_grid(resB['primary_pos'], resB['grid_shape'], resB['det_mm'])\n",
    "        plot_pred_true(resB['grid_final'], true_gridB, resB['det_mm'], reshape_size=RESHAPE_SIZE, min_val=PLOT_MIN_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e83d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Export helpers: track voxels & centers -----------------------------------\n",
    "def flat_to_ijk(flat_idx: int, grid_shape: Tuple[int,int,int]):\n",
    "    nx, ny, nz = grid_shape\n",
    "    i = flat_idx // (ny * nz)\n",
    "    j = (flat_idx % (ny * nz)) // nz\n",
    "    k = flat_idx % nz\n",
    "    return i, j, k\n",
    "\n",
    "def voxels_to_mm_centers(track_flat: List[int], grid_shape: Tuple[int,int,int], det_mm: Tuple[float,float,float]):\n",
    "    centers = []\n",
    "    for f in track_flat:\n",
    "        ijk = flat_to_ijk(f, grid_shape)\n",
    "        centers.append(voxel_center_ijk(ijk, grid_shape, det_mm))\n",
    "    return np.vstack(centers) if centers else np.empty((0,3))\n",
    "\n",
    "# Example: dump track centers for Variant A if available\n",
    "if len(files) > 0 and 'resA' in globals() and resA is not None:\n",
    "    centersA = voxels_to_mm_centers(resA['track_flat'], resA['grid_shape'], resA['det_mm'])\n",
    "    np.save(os.path.join(OUTPUT_DIR, 'track_centers_variantA.npy'), centersA)\n",
    "    LOGGER.info(f\"Saved Variant A track centers: {os.path.join(OUTPUT_DIR, 'track_centers_variantA.npy')} ({centersA.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2123ab",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Next Steps\n",
    "\n",
    "- **Fraction removal (`ALPHA_MODE`)**  \n",
    "  - `uniform`: each voxel along a photon path gets equal share (1/N).  \n",
    "  - `length`: each voxel's share is proportional to the path length inside that voxel (recommended).  \n",
    "  The peeling removes `ALPHA_SCALE * share` of the photon's weight for photons hitting the selected voxel.\n",
    "\n",
    "- **Multiple tracks**  \n",
    "  To extract multiple tracks, iterate `run_peeling(...)` again on the **residual** state after the first run,\n",
    "  recording the second track, and so on until stopping. (Left simple here to start with one.)\n",
    "\n",
    "- **Performance**  \n",
    "  This design does **not** rebuild the grid each iteration; it updates incrementally using the\n",
    "  precomputed incidence. For very large datasets (>10k photons per file), consider running in a\n",
    "  Python process instead of a notebook cell to avoid UI overhead. You can also batch files.\n",
    "\n",
    "- **Debug maps**  \n",
    "  If you want to inspect the photon↔voxel incidence, we can easily add exporters for the CSR arrays.\n",
    "\n",
    "- **Safeguards**  \n",
    "  We clamp the grid to ≥ 0 after updates to avoid negative drift. Min-support and remaining-weight\n",
    "  stops prevent runaway loops.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
